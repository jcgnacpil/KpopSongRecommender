{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f17153da-a343-4a53-b4a6-203af6f967e6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Song Gathering  \n",
    "**JC Nacpil 2021/09/06**\n",
    "\n",
    "In this notebook, we will build a database of Kpop songs with audio features using the Spotify Web API and Spotipy package. The output files will be used for `KpopSongRecommender`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71aae9da-e6b0-487d-b0c3-d4400c198fd2",
   "metadata": {},
   "source": [
    "## Set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60709ec-8ea3-4b12-8a59-1b098ce9afc8",
   "metadata": {},
   "source": [
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3ab09f-a2a4-4bb9-b5e4-2a06347ece8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library for accessing Spotify API\n",
    "import spotipy\n",
    "import spotipy.util as util\n",
    "from spotipy.oauth2 import SpotifyClientCredentials\n",
    "from spotipy.oauth2 import SpotifyOAuth\n",
    "\n",
    "# Scientific and vector computation for python\n",
    "import numpy as np\n",
    "\n",
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "\n",
    "# Library for this notebook providing utilitiy functions\n",
    "from utils import repeatAPICall\n",
    "\n",
    "# Progress bar\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Cosine similarity calculation\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Deep copy of python data structures\n",
    "from copy import deepcopy\n",
    "\n",
    "# Plotting library\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58463b1-32a3-4f2a-891c-ceab4982ce26",
   "metadata": {},
   "source": [
    "### Setting up Spotify API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209d007c-049e-4ee1-b6f7-8927e288e2ae",
   "metadata": {},
   "source": [
    "The following are the Spotiy API credentials `CLIENT_ID` and `CLIENT_SECRET` for our application. This allows us to access data from Spotify through the <a href='https://developer.spotify.com/documentation/web-api/'>Web API</a>. It is recommended to register your own application and manage these credentials at <a href = \"https://developer.spotify.com/dashboard/\">My Dashboard</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d39abb-b008-41cd-b65f-0c8b3a884bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIENT_ID = \"dc7ef763416f49aca20c740e46bd1f79\"\n",
    "CLIENT_SECRET = \"056f146106544a828574e8e903286fb7\"\n",
    "\n",
    "token = SpotifyClientCredentials(client_id=CLIENT_ID, client_secret=CLIENT_SECRET)\n",
    "cache_token = token.get_access_token(as_dict = False)\n",
    "sp = spotipy.Spotify(cache_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febf99e3-648d-43d3-be87-047495356ad1",
   "metadata": {},
   "source": [
    "### Utility functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50910417-060a-4755-831a-566d9cc3ea3f",
   "metadata": {},
   "source": [
    "For this notebook, we will use `repeatAPICall`, which is a function that repeatedly makes API calls until a successful request is reached. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4b4563-3318-4c01-9fec-ef7f6ae0a73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeatAPICall(func, args, max_retry = 5):\n",
    "    \"\"\"\n",
    "    Repeatedly calls spotipy func until a successful API request is made.\n",
    "    \n",
    "    Parameters:\n",
    "        func : func\n",
    "            Spotipy client function for making api calls\n",
    "        args: dict\n",
    "            Arguments to pass to func; Key: parameter, Value: parameter value\n",
    "            Check Spotipy API of specified func for details\n",
    "        max_retry: int\n",
    "            Maximum iterations before prompting user to retry or skip\n",
    "        \n",
    "    Returns:\n",
    "        result: dict\n",
    "            Result of a successful API call, or none\n",
    "        success: bool\n",
    "            True if API call is successful, False otherwise\n",
    "    \"\"\"\n",
    "\n",
    "    success = False\n",
    "    res = None\n",
    "    \n",
    "    i = 0\n",
    "    while i < max_retry:\n",
    "        try: \n",
    "            res = func(**args)\n",
    "            success = True\n",
    "            return res, success\n",
    "        except:\n",
    "            print(\"Error in API call; retrying\")\n",
    "            i += 1\n",
    "            pass\n",
    "        \n",
    "        if i >= max_retry:\n",
    "            reset_loop = input(\"Max retry limit reached. Retry {} more time/s?\".format(max_retry)).upper()\n",
    "            reset_loop = True if reset_loop == 'Y' else False\n",
    "            \n",
    "            # Reset the index of the loop if user chooses to reset\n",
    "            i = 0 if reset_loop else max_retry\n",
    "    return res, success"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20ecd7d-31a1-493c-9bf2-1e38c5b14ea2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 1: Getting playlists of a given category"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212893d3-1c6e-4b16-aef2-244421f41de1",
   "metadata": {},
   "source": [
    "In this step, we will gather playlists that are categorized as k-pop. We can use this as a starting point to gather an initial list of kpop artists."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606ef096-1eab-4af0-ab35-d173c8570641",
   "metadata": {},
   "source": [
    "This cell gets the list of playlist categories (with ID) available in Spotify. Let's set the country code to PH so we can get PH-specific results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e20b2d-4680-4b1e-8f88-4e57c8e77e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_categories = sp.categories(limit = 50, country = 'PH')\n",
    "\n",
    "categories = all_categories['categories']['items']\n",
    "for cat in categories:\n",
    "    print(\"Category: {} | ID : {}\".format(cat['name'],cat['id']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad42ae8-8651-411b-9ae6-45c651ce7027",
   "metadata": {},
   "source": [
    "This indicates that K-pop categories has `id = kpop`!\n",
    "\n",
    "Note for the future implementation: OPM has `id = opm`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6586b7a2-cdb2-4d54-a381-d495dc4730ad",
   "metadata": {},
   "source": [
    "This next cell gathers the playlists for the `kpop` category and saves it to a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772af6b9-5234-447a-80d4-1b0c0ca2dc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "kpop_playlists_result = sp.category_playlists('kpop', country='PH', limit = 50)\n",
    "kpop_playlists = kpop_playlists_result['playlists']['items']\n",
    "while kpop_playlists_result['playlists']['next']:\n",
    "    kpop_playlists_result = sp.next(kpop_playlists_result['playlists'])\n",
    "    kpop_playlists.extend(kpop_playlists_result['playlists']['items'])\n",
    "\n",
    "playlists = [playlist['name'] for playlist in kpop_playlists]\n",
    "playlist_uris = [playlist['uri'] for playlist in kpop_playlists]\n",
    "\n",
    "playlist_df = pd.DataFrame(zip(playlists,playlist_uris),columns = [\"playlist\", \"playlist_uri\"]).drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f250e920-f623-43c5-b016-e89f197c7b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the playlist list to .csv file\n",
    "filename = 'Data/playlists.csv'\n",
    "playlist_df.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8f09d6-0353-481e-84d9-41efd7b72fb2",
   "metadata": {},
   "source": [
    "## Step 2: Collecting artists from playlists"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90a549b-86ff-4d8c-92a7-468df68f8432",
   "metadata": {},
   "source": [
    "In this step, we will use the list of playlists and gather all the artists that appear in them. Since we're using k-pop playlists, we assume that most of the artists we get from this step are k-pop. \n",
    "\n",
    "**Note:** Usually there will be some non-kpop artists appearing in this list, such as Dua Lipa or Ed Sheeran. These are usually artists that appear on k-pop collabs (ex. Dua Lipa and BLACKPINK - Kiss and Make Up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a7ea9f-cb55-4699-b679-1033d1d80782",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the existing playlist data\n",
    "playlist_dir = 'Data/playlists.csv'\n",
    "playlist_df = pd.read_csv(playlist_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a884310-5c5e-446e-89ad-9386fecc2336",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loop through playlists to build a list of artists\n",
    "\n",
    "# Get the list of unique identifiers for each playlist\n",
    "playlists = playlist_df.playlist.values\n",
    "playlist_uris = playlist_df.playlist_uri.values\n",
    "\n",
    "# Create dataframe to store artist data\n",
    "artist_cols = ['artist','artist_uri']\n",
    "artist_df = pd.DataFrame(columns = artist_cols)\n",
    "\n",
    "for playlist,uri in zip(playlists, playlist_uris):\n",
    "    \n",
    "    print(\"Current playlist: {}\".format(playlist))\n",
    "    \n",
    "    playlist_result, success = repeatAPICall(sp.playlist_tracks,{'playlist_id':uri})\n",
    "    if not success: \n",
    "        print(\"Error in playlist {}\".format(playlist))\n",
    "        continue\n",
    "\n",
    "    # Remove value in playlist_result['items']  when track is listed as None object\n",
    "    playlist_result['items'] = [track for track in playlist_result['items'] if track['track'] is not None]\n",
    "\n",
    "    # Skip the playlist if there are any errors\n",
    "    try:\n",
    "        artist_uris = [track['track']['artists'][0]['uri'] for track in playlist_result['items']]\n",
    "        artists = [track['track']['artists'][0]['name'] for track in playlist_result['items']]\n",
    "    except: \n",
    "        print(\"Error in playlist {}\".format(playlist))\n",
    "    \n",
    "    temp_df = pd.DataFrame(zip(artists,artist_uris),columns = artist_cols)\n",
    "    artist_df = pd.concat([artist_df.reset_index(drop=True), temp_df.reset_index(drop=True)]).drop_duplicates()\n",
    "\n",
    "# Reset the index of our resulting dataframe\n",
    "artist_df = artist_df.drop_duplicates().reset_index(drop=True)\n",
    "artist_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057ef66f-2425-4304-b9f1-c18d7c91bd84",
   "metadata": {},
   "source": [
    "At this point, we now have 900~ artists in our database! We save our current output as `artists.csv`. In the next succeeding cells, we will extend the list by getting related acts for every artist in our current list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87a55b9-e873-4521-8ea1-6b2ea3db4fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the existing artist data\n",
    "artists_dir = 'Data/artists.csv'\n",
    "artist_df.to_csv(artists_dir, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d828470-a220-49c1-8600-b50c196dcc59",
   "metadata": {},
   "source": [
    "## Step 3: Extending artist data by gathering related artists"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa9a8a6-b998-4ecf-aea0-b4ee61e74e21",
   "metadata": {},
   "source": [
    "In this step, we will extended our current list of artists by adding related artists to our current list. This will run for a set number of iterations, so after getting the initial list of related artists, we can get then gather even more artists from this new batch. \n",
    "\n",
    "There are two important steps to improve runtime and avoid repeating processes. First, we label each artist with `temp_processed` (bool), which indicates whether we have already processed that artist's related artists. We set this initally to `False` and update it to `True` when an iteration has finished. \n",
    "\n",
    "Second, we only filter out artists of certain genres that we are interested in. `sp.artist_related_artists()` returns 20 related artists for a given artist, which can blow up our list exponentially and add artists that we don't want. For example, `BLACKPINK` and `BTS` are related to `Dua Lipa` and `Halsey`, respectively, as they feature together on collabs. However, if we keep these two results and get additional related artists based on them, we are likely to get more pop artists (~20) unrelated to the genre we are looking for. `genre_filter` is a list of substrings that we use to match to an artist's own genre list to decide whether to keep that artist in our list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb967ac8-35a6-438e-b875-5bc3a7791338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the existing artist data to extend\n",
    "# For testing: randomly sample rows\n",
    "artists_dir = 'Data/artists.csv'\n",
    "artist_extended_df = pd.read_csv(artists_dir)\n",
    "artist_extended_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39fcf6c-9c02-4c20-bd6e-7b4483191615",
   "metadata": {},
   "source": [
    "In the next cell, the code will go through each artist and get a list of related artists. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29bf82d-a635-4f37-bdf1-8824cb733632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a 'processed' column to artist_df indicating if its already been processed by this loop\n",
    "artist_extended_df['temp_processed'] = False\n",
    "\n",
    "# Filter for genres\n",
    "# We use 'k-pop' and 'k-rap' as the genre substrings\n",
    "# We can also add 'korean' to match korean artists that are not considered k-pop (ex. OSTs)\n",
    "genre_filter = ['k-pop', 'k-rap']\n",
    "# genre_filter = ['k-','korean']\n",
    "\n",
    "# Keep track of iteration progress (see sense check section below)\n",
    "artist_count = [len(artist_extended_df.artist_uri.values)]\n",
    "iter_count = [0]\n",
    "removed_count = [0]\n",
    "\n",
    "# Set maximum iterations\n",
    "max_iter = 15\n",
    "\n",
    "for i in range(max_iter):\n",
    "    print(\"Current iter: {}\".format(i+1))\n",
    "    rel_artists = []\n",
    "    rel_artist_uris = []\n",
    "    \n",
    "    # Create temporary df to score artists to be processed\n",
    "    temp_df = artist_extended_df.copy()\n",
    "    temp_df = temp_df[temp_df.temp_processed == False]\n",
    "    \n",
    "     # If temp df is empty, end the loop\n",
    "    if temp_df.empty:\n",
    "        print(\"No more artists to be processed! Breaking loop.\")\n",
    "        break\n",
    "    \n",
    "    artists = temp_df.artist.values\n",
    "    artist_uris = temp_df.artist_uri.values\n",
    "    \n",
    "    print(\"Iter: {} | Artists count: {} | To be processed : {}\".format(i+1, len(artist_extended_df.artist_uri.values), len(artist_uris) ))\n",
    "    \n",
    "    # Track number of artists removed per iteration\n",
    "    total_removed = 0\n",
    "    \n",
    "    # Loop through artists \n",
    "    for artist,uri in zip(artists,artist_uris):\n",
    "        \n",
    "        # Get related artists for the current artist\n",
    "        rel_artists_result, success = repeatAPICall(sp.artist_related_artists,{'artist_id':uri})\n",
    "        if not success: \n",
    "            print(\"Skipping to next artist.\")\n",
    "            continue    \n",
    "        \n",
    "        # Remove artists whose genres do not contain the substrings in genre_filter\n",
    "        old_count = len(rel_artists_result['artists'])\n",
    "        rel_artists_result['artists'] = [rel_artist for rel_artist in rel_artists_result['artists'] if any(genre in ''.join(rel_artist['genres']) for genre in genre_filter)]\n",
    "        new_count = len(rel_artists_result['artists'])\n",
    "        \n",
    "        # Track number of removed artists\n",
    "        removed = old_count - new_count\n",
    "        total_removed += removed\n",
    "        \n",
    "        rel_artists.extend([artist['name'] for artist in rel_artists_result['artists']])\n",
    "        rel_artist_uris.extend([artist['uri'] for artist in rel_artists_result['artists']])\n",
    "\n",
    "    # Create dataframe of related artists that were gathered\n",
    "    rel_artist_df = pd.DataFrame(zip(rel_artists,rel_artist_uris),columns = [\"artist\", \"artist_uri\"]).drop_duplicates()\n",
    "    rel_artist_df['temp_processed'] = False\n",
    "    \n",
    "    # At this step, all the entries in artist_df has been processed and labelled accordingly\n",
    "    artist_extended_df['temp_processed'] = True\n",
    "    \n",
    "    # Combine artist_extended_df and rel_artist_df\n",
    "    # Drop duplicates and keep first value\n",
    "    # This ensures that we keep the firtst duplicate songs \n",
    "    # between artist and rel_artist_df (with different temp_processed values) \n",
    "    \n",
    "    artist_extended_df = pd.concat([artist_extended_df.reset_index(drop=True), rel_artist_df.reset_index(drop=True)]).drop_duplicates(subset = ['artist', 'artist_uri'], keep = 'first')\n",
    "    \n",
    "    # Add metrics to array\n",
    "    iter_count.append(i+1)\n",
    "    artist_count.append(len(artist_extended_df.artist_uri.values))\n",
    "    removed_count.append(total_removed)\n",
    "\n",
    "print(\"Done! Final count: {}\".format(artist_count[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2bf9dd-4680-4d38-a3e7-168bdd14ff43",
   "metadata": {},
   "source": [
    "Here's our final list of artists!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68c743b-6b22-46be-a6a3-f88327a60ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "artist_extended_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25361197-d6a7-451d-8ab0-c8d9fe78d399",
   "metadata": {},
   "source": [
    "### Sense Check\n",
    "This cell plots the total number of artists gathered (blue) and related artists removed (orange) as function of the number of iterations. We see that the blue line generally plateaus, indicating that we reached a reasonable upper limit of possible artists gathered. We also see that the number of artists removed is large for `i = 1` at around (8000~). This means that the first iteration removes a large number of non-kpop artists. Without removing these artists per iteration, the loop will not converge to a finite list of artists. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17ffe19-e1d1-4bf1-a1c3-69c4ce34bddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(iter_count,artist_count, label = \"Artists in list\")\n",
    "plt.plot(iter_count[1:], removed_count[1:], label = \"Removed in processing (not in genre filter)\")\n",
    "plt.xlabel(\"Number of iterations\")\n",
    "plt.ylabel(\"Artist count\")\n",
    "plt.legend()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0158e763-396c-44b0-995f-6a8cede7fe26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the existing artist data\n",
    "# We drop the temp_processed column before writing to csv\n",
    "artists_extended_dir = 'Data/artists_extended.csv'\n",
    "artist_extended_df.drop('temp_processed', axis = 1).to_csv(artists_extended_dir, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbbcaf6-3108-4ba4-a604-08614d942082",
   "metadata": {},
   "source": [
    "## Step 4: Loading top tracks per artist\n",
    "From our list of k-pop artists, we then get their top 10 tracks. This gives us a reasonable number of songs for our Kpop Song Recommender! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8f7ca3-bc3e-42a9-8db5-b12ed3ec8c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the existing artist data \n",
    "artists_dir = 'Data/artists.csv' \n",
    "artists_dir = 'Data/artists_extended.csv' # Uncomment this if you want to use the extended artists dataset\n",
    "artist_df = pd.read_csv(artists_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b605ef6-28e1-4a9e-9f77-61c0affc87fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "artists = artist_df.artist.values\n",
    "artist_uris = artist_df.artist_uri.values\n",
    "\n",
    "# Loop through artist to build a list of tracks from their top 10 songs\n",
    "\n",
    "# Create dataframe to store artist data\n",
    "artist_cols = ['artist', 'artist_uri']\n",
    "track_cols = ['track','track_uri','popularity']\n",
    "track_df = pd.DataFrame(columns = artist_cols + track_cols)\n",
    "\n",
    "for artist,uri in tqdm(zip(artists, artist_uris), total = len(artist_uris)):\n",
    "    \n",
    "    # print(\"Current artist: {}\".format(artist))\n",
    "    \n",
    "    top10_result, success = repeatAPICall(sp.artist_top_tracks,{'artist_id':uri,'country':'PH'})\n",
    "    if not success: \n",
    "        print(\"Skipping to next artist.\")\n",
    "        continue\n",
    "\n",
    "    # Remove value in playlist_result['items']  when track is listed as None object\n",
    "    #top10_result['tracks'] = [track for track in top10_result['tracks'] if track is not None]\n",
    "\n",
    "    # Skip the playlist if there are any errors\n",
    "    try:\n",
    "        track_uris = [track['uri'] for track in top10_result['tracks']]\n",
    "        tracks = [track['name'] for track in top10_result['tracks']]\n",
    "        popularity = [track['popularity'] for track in top10_result['tracks']]\n",
    "    except: \n",
    "        print(\"Error in playlist {}\".format(playlist))\n",
    "    \n",
    "    temp_df = pd.DataFrame(zip(tracks,track_uris, popularity),columns = track_cols)\n",
    "    # Set the artist and artist columns\n",
    "    temp_df['artist'] = artist\n",
    "    temp_df['artist_uri'] = uri\n",
    "    track_df = pd.concat([track_df.reset_index(drop=True), temp_df.reset_index(drop=True)]).drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256bb70b-cddd-4993-aff2-2cda29fda90c",
   "metadata": {},
   "source": [
    "Note: if a track has multiple artists, and those artists have this track, it will show up multiple times. In the cell below, we see that the number of rows is more than the number of unique track_uri. \n",
    "\n",
    "We will keep these duplicates for now, but keep this in mind when post-processing the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2bc261-d6f3-4065-907c-49a79204b66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of track_df rows: {}\\nNumber of unique track_uri: {}\".format(len(track_df), track_df['track_uri'].nunique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df98925-2f4a-4d7e-a950-331f6e9c4b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all tracks to file\n",
    "tracks_dir = 'Data/tracks_top10.csv'\n",
    "track_df.to_csv(tracks_dir, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12580533-655b-4098-942d-1cd62bf5b79f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 5: Getting audio features per track\n",
    "\n",
    "In this last step, we will generate the audio features for each track in our database using Spotify's Audio Features functionality. These include a song's danceability, tempo, energy key, time_signature, liveness, etc. In the main notebook, this will be used to as a basis to recommend kpop songs that are similar to a user's top tracks in terms of these features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d75c4d-1d82-4791-b7f5-f78e339f991b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the track data\n",
    "tracks_dir = 'Data/tracks_top10.csv'\n",
    "track_df = pd.read_csv(tracks_dir)\n",
    "\n",
    "tracks = track_df.track.values\n",
    "track_uris = track_df.track_uri.values\n",
    "\n",
    "track_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c6c300-5ded-4adf-a287-f6c8b90abbb2",
   "metadata": {},
   "source": [
    "In this next cell, we will go through each track and generate its audio features (saved as a dataframe). Each track is identified by its unique `track_uri`.\n",
    "\n",
    "`sp.audio_feautures()` takes a list of track_ids (maximum of 100). We will loop through the list of track uri in batches of 100 to minimize the amount of API requests. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada54236-aba7-46d9-ba6f-d4f381378758",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "\n",
    "# This list of columns is taken directly from the keys of a feature dictionary\n",
    "features_cols = ['danceability', \n",
    "                 'energy', \n",
    "                 'key', \n",
    "                 'loudness', \n",
    "                 'mode', \n",
    "                 'speechiness', \n",
    "                 'acousticness', \n",
    "                 'instrumentalness', \n",
    "                 'liveness', \n",
    "                 'valence', \n",
    "                 'tempo', \n",
    "                 'type', \n",
    "                 'id', \n",
    "                 'uri', \n",
    "                 'track_href', \n",
    "                 'analysis_url', \n",
    "                 'duration_ms', \n",
    "                 'time_signature']\n",
    "\n",
    "features_df = pd.DataFrame(columns = features_cols)\n",
    "\n",
    "for i in tqdm(range(0, len(track_uris), batch_size)):\n",
    "    \n",
    "    # Select the current batch\n",
    "    track_uris_batch = track_uris[i:i+batch_size]\n",
    "    \n",
    "    features_result, success = repeatAPICall(sp.audio_features,{'tracks':track_uris_batch})\n",
    "    if not success: \n",
    "        print(\"Skipping to next batch.\")\n",
    "        continue\n",
    "    \n",
    "    # Deepcopy the list of dictionaries to be modified\n",
    "    # This is necessary for this particular structure\n",
    "    features_dicts = deepcopy(features_result)\n",
    "    \n",
    "    \n",
    "    # Drop None in features_dict\n",
    "    # This will mean that some of our songs will not have features\n",
    "    if any(d is None for d in features_dicts):\n",
    "        \n",
    "        print(\"Batch: {} to {} | Some songs do not have features; dropping from list.\".format(i+1, i+1+batch_size)) \n",
    "        print(\"Count: {}\".format(len(features_dicts)))\n",
    "        features_dicts = [d for d in features_dicts if d is not None]\n",
    "        print(\"New count: {}\".format(len(features_dicts)))\n",
    "\n",
    "    temp_df = pd.DataFrame.from_records(features_dicts) \n",
    "    features_df = pd.concat([features_df.reset_index(drop=True), temp_df.reset_index(drop=True)])\n",
    "    \n",
    "    temp_df_count = len(temp_df.index)\n",
    "    if temp_df_count != batch_size:\n",
    "        print(\"Batch: {} to {} | Dataframe rows count: {}\".format(i+1, i+1+batch_size, temp_df_count)) \n",
    "\n",
    "\n",
    "# Reset index and rename 'uri' to 'track_uri'\n",
    "# Drop duplicates based on track_uri\n",
    "features_df = features_df.rename(columns={'uri':'track_uri'}).drop_duplicates(subset=['track_uri'])\n",
    "features_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b796470e-848c-4348-bc1f-68965792443a",
   "metadata": {},
   "source": [
    "Finally, we left join the features to the `track_df` using `track_uri`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c582d700-cbb9-42da-bb0a-f8969216da65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge features to track_df by track_uri\n",
    "# Note: some rows will not have features. We keep them for now to retain the track info\n",
    "track_features_df = track_df.merge(features_df, on='track_uri', how='left').reset_index(drop = True)\n",
    "track_features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd3e61d-6c84-4180-a9d8-92fa8908bd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save tracks with features to file\n",
    "# Save all tracks to file\n",
    "tracks_features_dir = 'Data/tracks_top10_features.csv'\n",
    "track_features_df.to_csv(tracks_features_dir, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36f436e-a352-4626-8b16-2b38150ea180",
   "metadata": {},
   "source": [
    "## Done!\n",
    "\n",
    "After running this notebook, you should now have the following updated files in your Data Folder:\n",
    "1. playlists.csv\n",
    "2. artists.csv\n",
    "3. artists_extended.csv\n",
    "4. tracks_top10.csv\n",
    "5. tracks_top10_features.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf2d105-37dd-4d25-9b4f-70ffaf222a3e",
   "metadata": {},
   "source": [
    "The following code cells will try to load all albums by an artist. This will be more computatinally expensive than the previous segment where we only got an artist's top 10 tracks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729cc831-16be-48ae-8f2f-f6bed814baad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the existing artist data \n",
    "artists_dir = 'Data/artists.csv' \n",
    "artists_dir = 'Data/artists_extended.csv' # Uncomment this if you want to use the extended artists dataset\n",
    "artist_df = pd.read_csv(artists_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e993d1-6f3d-4155-8a20-d65a58d69e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "artists = artist_df.artist.values\n",
    "artist_uris = artist_df.artist_uri.values\n",
    "\n",
    "# Loop through artist to build a list of albums\n",
    "\n",
    "# Create dataframe to store artist data\n",
    "artist_cols = ['artist', 'artist_uri']\n",
    "album_cols = ['album','album_uri','release_date','release_date_precision','total_tracks']\n",
    "album_df = pd.DataFrame(columns = artist_cols + album_cols)\n",
    "\n",
    "for artist,uri in tqdm(zip(artists, artist_uris), total = len(artist_uris)):\n",
    "    \n",
    "    func_params = {\n",
    "        'artist_id':uri,\n",
    "        'country':'PH',\n",
    "        'album_type':['album','single','compilation']\n",
    "    }\n",
    "    \n",
    "    albums_result, success = repeatAPICall(sp.artist_albums, func_params)\n",
    "    \n",
    "    if not success: \n",
    "        print(\"Skipping to next artist.\")\n",
    "        continue\n",
    "\n",
    "    albums_list = albums_result['items']\n",
    "    while albums_result['next']:\n",
    "#         print(\"Going to next 50 albums.\")\n",
    "        albums_result, success = repeatAPICall(sp.next,{'result':albums_result})\n",
    "        if not success: \n",
    "            print(\"Skipping to next artist.\")\n",
    "            continue\n",
    "        albums_list.extend(albums_result['items'])\n",
    "\n",
    "    # Skip the artist if there are any errors\n",
    "    try:\n",
    "        album_uris = [album['uri'] for album in albums_list]\n",
    "        albums = [album['name'] for album in albums_list]\n",
    "        release_dates = [album['release_date'] for album in albums_list]\n",
    "        release_date_precisions = [album['release_date_precision'] for album in albums_list]\n",
    "        totals = [album['total_tracks'] for album in albums_list]\n",
    "\n",
    "    except: \n",
    "        print(\"Error in artist {}\".format(artist))\n",
    "    \n",
    "    temp_df = pd.DataFrame(zip(albums,album_uris, release_dates, release_date_precisions, totals),columns = album_cols)\n",
    "    # Set the artist and artist columns\n",
    "    temp_df['artist'] = artist\n",
    "    temp_df['artist_uri'] = uri\n",
    "    \n",
    "    album_df = pd.concat([album_df.reset_index(drop=True), temp_df.reset_index(drop=True)]).drop_duplicates()\n",
    "album_df = album_df.reset_index(drop = True)\n",
    "\n",
    "album_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056413d0-f143-4c85-ae9d-cc7bfb7062f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "album_df.sample(50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
